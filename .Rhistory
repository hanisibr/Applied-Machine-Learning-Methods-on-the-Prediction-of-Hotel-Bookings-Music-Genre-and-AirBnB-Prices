# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
install.packages("reticulate")
install.packages("tensorflow")
library(tensorflow)
install.packages("reticulate")
library(tensorflow)
library(keras)
library(caret)
library(dplyr)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
detach("package:tensorflow", unload = TRUE)
install.packages("reticulate")
install.packages("tensorflow")
library(tensorflow)
install.packages("reticulate")
install.packages("tensorflow")
library(tensorflow)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(dplyr)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(tensorflow)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(keras)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
install.packages("tensorflow")
install.packages("tensorflow")
library(keras)
library(tensorflow)
library(dplyr)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
detach("package:tensorflow", unload = TRUE)
library(tensorflow)
detach("package:keras", unload = TRUE)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(keras)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(keras)
install.packages("reticulate")
install.packages("tensorflow")
library(tensorflow)
install.packages("tensorflow")
install.packages("reticulate")
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(dplyr)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam(),
metrics = c("mean_absolute_error")
)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
keras::install_keras(tensorflow = "default")
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(keras)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(selected_features)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam(),
metrics = c("mean_absolute_error")
)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
reticulate::py_last_error()
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
str(x_train)
str(y_train)
View(x_train)
x_train <- as.numeric(x_train)
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
y_train <- as.numeric(y_train)
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
nrow(x_train)
nrow(y_train)
x_train <- as.matrix(x_train)
y_train <- as.matrix(y_train)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
train_ratio <- 0.8
num_train_samples <- nrow(selected_features) * train_ratio
x_train <- selected_features[1:num_train_samples, ]
y_train <- prices[1:num_train_samples]
x_test <- selected_features[(num_train_samples + 1):nrow(selected_features), ]
y_test <- prices[(num_train_samples + 1):nrow(selected_features)]
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
x_train <- as.numeric(x_train)
y_train <- as.numeric(y_train)
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
x_train <- as.matrix(x_train)
y_train <- as.matrix(y_train)
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
reticulate::py_last_error()
train_ratio <- 0.8
num_train_samples <- nrow(selected_features) * train_ratio
x_train <- selected_features[1:num_train_samples, ]
y_train <- prices[1:num_train_samples]
x_test <- selected_features[(num_train_samples + 1):nrow(selected_features), ]
y_test <- prices[(num_train_samples + 1):nrow(selected_features)]
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
str(x_train)
x_train <- as.numeric(x_train)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
nrow(x_train)
nrow_x_train <- nrow(x_train)
nrow_y_train <- nrow(y_train)
# Ensure x_train and y_train have the same number of samples
if (nrow_x_train != nrow_y_train) {
stop("Number of samples in x_train and y_train do not match.")
}
if (nrow_x_train != nrow_y_train) {
stop("Number of samples in x_train and y_train do not match.")
}
install.packages("tensorflow")
install.packages("tensorflow")
install.packages("tensorflow")
install.packages("tensorflow")
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(dplyr)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(keras)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
library(tensorflow)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam(),
metrics = c("mean_absolute_error")
)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
# Ensure x_train and y_train have the same number of samples
if (nrow_x_train != nrow_y_train) {
stop("Number of samples in x_train and y_train do not match.")
}
x_train <- as.matrix(x_train)
y_train <- as.matrix(y_train)
x_train <- as.numeric(x_train)
y_train <- as.numeric(y_train)
nrow_x_train <- nrow(x_train)
nrow_y_train <- nrow(y_train)
if (nrow_x_train != nrow_y_train) {
stop("Number of samples in x_train and y_train do not match.")
}
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 36,
validation_split = 0.2
)
set.seed(42)  # Set a seed for reproducibility
train_ratio <- 0.8
# Sample row indices for training set
train_indices <- sample(1:nrow(selected_features), size = round(train_ratio * nrow(selected_features)), replace = FALSE)
# Split the data into training and testing sets
x_train <- data[train_indices, -target_column_index]  # Exclude the target column from x_train
x_train <- data[train_indices, -prices]  # Exclude the target column from x_train
x_train <- data[train_indices, -price]  # Exclude the target column from x_train
x_train <- selected_features[train_indices, -prices]  # Exclude the target column from x_train
y_train <- selected_features[train_indices, prices]
x_train <- selected_features[1:num_train_samples, ]
y_train <- prices[1:num_train_samples]
x_test <- selected_features[(num_train_samples + 1):nrow(selected_features), ]
y_test <- prices[(num_train_samples + 1):nrow(selected_features)]
# Sample row indices for training set
train_indices <- sample(1:nrow(selected_features), size = round(train_ratio * nrow(selected_features)), replace = FALSE)
x_train <- selected_features[train_indices, -prices]  # Exclude the target column from x_train
y_train <- selected_features[train_indices, prices]
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -1:train_indices]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -train_indices]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -1]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices,-1]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices,-[1]]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices,:-1]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -(-1)]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -(1)]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, (-1)]  # Exclude the target column from x_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -(-1)]  # Exclude the target column from x_train
y_train <- selected_features[train_indices, -1]   # Extract the target column into y_train
y_train <- selected_features[train_indices, (-1)]   # Extract the target column into y_train
y_train <- selected_features[train_indices, ]   # Extract the target column into y_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -(-1)]  # Exclude the target column from x_train
y_train <- !x_train   # Extract the target column into y_train
y_train <- selected_features[train_indices, !(-1)] # Extract the target column into y_train
View(y_train)
# Split the data into training and testing sets
x_train <- selected_features[train_indices, 1]  # Exclude the target column from x_train
y_train <- selected_features[train_indices, -(-1)] # Extract the target column into y_train
# Split the data into training and testing sets
x_train <- selected_features[train_indices, -(-1)]  # Exclude the target column from x_train
x_train <- selected_features[train_indices, -(-1)]  # Exclude the target column from x_train
y_train <- selected_features[train_indices, 1]
x_test <- data[-train_indices, -(-1)]  # Exclude the target column from x_test
x_test <- selected_features[-train_indices, -(-1)]  # Exclude the target column from x_test
y_test <- selected_features[-train_indices, 1]
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam(),
metrics = c("mean_absolute_error")
)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 36,
validation_split = 0.2
)
x_train <- array_reshape(x_train, c(nrow(x_train), 1))
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
str(x_train)
str(y_train)
x_train <- as.numeric(x_train)
y_train <- as.numeric(y_train)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam(),
metrics = c("mean_absolute_error")
)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
x_train <- as.matrix(x_train)
y_train <- as.matrix(y_train)
# Define the MLP model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)  # Output layer with a single neuron for price prediction
# Compile the model
model %>% compile(
loss = "mean_squared_error",
optimizer = optimizer_adam(),
metrics = c("mean_absolute_error")
)
# Train the model with selected features
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 32,
validation_split = 0.2
)
# Evaluate the model with selected features
evaluation <- model %>% evaluate(x_test, y_test)
x_test <- as.numeric(x_test)
y_test <- as.numeric(y_test)
x_test <- as.matrix(x_test)
y_test <- as.matrix(y_test)
# Evaluate the model with selected features
evaluation <- model %>% evaluate(x_test, y_test)
print(evaluation)
View(categorical_features)
View(numeric_features)
# Make predictions for new data using the selected features
# For new data, perform the same one-hot encoding for the categorical features
new_data <- selected_features.frame(minimum_nights = 0.1, availability_365 = -0.2, neighbourhood_group = "CategoryA", room_type = "CategoryX")
# Make predictions for new data using the selected features
# For new data, perform the same one-hot encoding for the categorical features
new_data <- data_without_blank_values.frame(minimum_nights = 0.1, availability_365 = -0.2, neighbourhood_group = "CategoryA", room_type = "CategoryX")
View(selected_features)
# Make predictions for new data using the selected features
# For new data, perform the same one-hot encoding for the categorical features
new_data <- data.frame(minimum_nights = 0.1, availability_365 = -0.2, neighbourhood_group = "CategoryA", room_type = "CategoryX")
new_data$feature1 <- (new_data$feature1 - feature_means[1]) / feature_stds[1]
new_data$feature2 <- (new_data$feature2 - feature_means[2]) / feature_stds[2]
feature_means <- colMeans(numeric_features)
feature_means <- colMeans(numeric_features)
numeric_features <- as.matrix(features[, c('minimum_nights', 'availability_365')])
feature_means <- colMeans(numeric_features)
# Preprocess the data (feature scaling for numerical features)
numeric_features <- as.numeric(as.matrix(features[, c('minimum_nights', 'availability_365')]))
feature_means <- colMeans(numeric_features)
new_data$feature1 <- (new_data$feature1 - feature_means[1]) / feature_stds[1]
feature_means <- colMeans(x_train[, c("minimum_nights", "availability_365")])
# Calculate means and standard deviations of feature1 and feature2
feature_means <- colMeans(x_train[, c("minimum_nights", "availability_365")])
View(x_train)
View(data_without_blank_values)
# Calculate means and standard deviations of feature1 and feature2
feature_means <- colMeans(selected_features[, c("minimum_nights", "availability_365")])
selected_features_numeric <- as.numeric(selected_features)
# Calculate means and standard deviations of feature1 and feature2
feature_means <- colMeans(selected_features_numeric[, c("minimum_nights", "availability_365")])
features <- as.numeric(features)
# Calculate means and standard deviations of feature1 and feature2
feature_means <- colMeans(features[, c("minimum_nights", "availability_365")])
numeric_features <- as.matrix(features[, c('minimum_nights', 'availability_365')])
# Extract features and target (price) from the filtered dataset
features <- as.matrix(data_without_blank_values[, c('minimum_nights', 'availability_365', 'neighbourhood_group', 'room_type')])
numeric_features <- as.matrix(features[, c('minimum_nights', 'availability_365')])
feature_means <- colMeans(numeric_features)
# Preprocess the data (feature scaling for numerical features)
numeric_features <- as.matrix(features[, c('minimum_nights', 'availability_365')])
feature_means <- as.numeric(colMeans(numeric_features))
# Preprocess the data (feature scaling for numerical features)
numeric_features <- as.matrix(features[, c('minimum_nights', 'availability_365')])
numeric_features <- as.numeric(numeric_features)
feature_means <- colMeans(numeric_features)
feature_means <- colMeans(numeric_features, 2)
# Convert categorical_feature to factor (assuming it is a character column)
new_data$categorical_feature <- as.factor(new_data$categorical_feature)
# For new data, perform the same one-hot encoding for the categorical features
new_data <- data.frame(neighbourhood_group = "CategoryA", room_type = "CategoryB")
new_data_encoded <- predict(dummy_data, newdata = new_data)
View(new_data)
# For new data, perform the same one-hot encoding for the categorical features
new_data <- categorical_features.frame(neighbourhood_group = "CategoryA", room_type = "CategoryB")
# For new data, perform the same one-hot encoding for the categorical features
new_data <- data_without_blank_values.frame(neighbourhood_group = "CategoryA", room_type = "CategoryB")
# For new data, perform the same one-hot encoding for the categorical features
new_data <- bbData.frame(neighbourhood_group = "CategoryA", room_type = "CategoryB")
# For new data, perform the same one-hot encoding for the categorical features
new_data <- data.frame(neighbourhood_group = "CategoryA", room_type = "CategoryB")
new_data_encoded <- predict(dummy_data, newdata = new_data)
levels(new_data$categorical_feature)
View(selected_features)
View(x_train)
View(y_train)
View(y_test)
View(x_train)
